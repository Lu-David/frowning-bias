{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%cd /content"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qxCGD9C322U",
    "outputId": "f7f69485-0969-4964-ec29-91e2c3effe07",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! rm -rf frowning-bias"
   ],
   "metadata": {
    "id": "YPhfw88p3uz-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "! git clone https://github.com/Lu-David/frowning-bias -b main"
   ],
   "metadata": {
    "id": "hM0NheQqwIKc",
    "outputId": "e50016e0-f66b-412c-f81b-51d4444ccd7d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'frowning-bias'...\n",
      "remote: Enumerating objects: 295, done.\u001B[K\n",
      "remote: Counting objects: 100% (295/295), done.\u001B[K\n",
      "remote: Compressing objects: 100% (204/204), done.\u001B[K\n",
      "remote: Total 295 (delta 164), reused 213 (delta 87), pack-reused 0\u001B[K\n",
      "Receiving objects: 100% (295/295), 3.11 MiB | 4.37 MiB/s, done.\n",
      "Resolving deltas: 100% (164/164), done.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd  frowning-bias"
   ],
   "metadata": {
    "id": "DCiAFvmU0glt",
    "outputId": "09557152-6957-49cc-e4f8-f853b729d10d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/frowning-bias\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from download_data import download_raf\n",
    "\n",
    "download_raf()"
   ],
   "metadata": {
    "id": "6txuqyLR0ywX",
    "outputId": "7b25f100-6ed8-4bf5-d1f1-45617d656fd5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading https://www.ugrad.cs.jhu.edu/~dlu17/aligned_test.zip to data/RAF/aligned_test.zip\n",
      "Unzipping data/RAF/aligned_test.zip\n",
      "Downloading https://www.ugrad.cs.jhu.edu/~dlu17/aligned_train.zip to data/RAF/aligned_train.zip\n",
      "Unzipping data/RAF/aligned_train.zip\n",
      "Downloading https://www.ugrad.cs.jhu.edu/~wwang136/manual.zip to data/RAF/manual.zip\n",
      "Unzipping data/RAF/manual.zip\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! python3 split.py"
   ],
   "metadata": {
    "id": "vmKDAkgp5rZK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "! mkdir -p ./results/baseline/"
   ],
   "metadata": {
    "id": "4XTyGx5q6ZRv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ! python3 train.py --architecture \"ResNet18\" --pretrained \"y\" --frozen \"n\" --batch-size 32 --max-epochs 100 --dropout 0.5 --print-batches \"y\" --results-dir \"./results/scratch\" "
   ],
   "metadata": {
    "id": "eMzdbjLl02yP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "! chmod +x -R scripts"
   ],
   "metadata": {
    "id": "5qq456AZWCv6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# ! python3 train_all_attrweight.py --break-patience 1 --results-file \"baseline\" --architecture \"ResNet18\" --pretrained \"y\" --frozen \"n\" --initial-lr 0.001 --optimizer-family \"AdamW\" --batch-size 32 --break-patience 3 --weight-decay 0.001 --dropout 0 --print-batches \"n\" --results-dir \"./results/baseline\"\n"
   ],
   "metadata": {
    "id": "_VtoqaKdT0ky",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# train baseline model\n",
    "! python3 train_all.py --results-file \"baseline\" --architecture \"ResNet18\" --pretrained \"y\" --frozen \"n\" --initial-lr 0.001 --optimizer-family \"AdamW\" --batch-size 32 --break-patience 3 --weight-decay 0.001 --dropout 0 --class-weights \"y\" --print-batches \"y\" --results-dir \"./results/baseline\"\n"
   ],
   "metadata": {
    "id": "7LaUzfphSqx8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ceed19e-c4f5-4c8c-b8cf-42e513da2419",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'model_type': 'ResNet18', 'pretrained': True, 'n_labels': 7, 'frozen': False, 'initial_lr': 0.001, 'batch_size': 32, 'max_epochs': 50, 'optimizer_family': 'AdamW', 'weight_decay': 0.001, 'momentum': 0.9, 'scheduler_family': 'step', 'drop_factor': 0.1, 'plateau_patience': 3, 'plateau_threshold': 0.0001, 'break_patience': 3, 'data_dir': './data', 'dataset': 'RAF', 'train_file': 'train_files.csv', 'val_file': 'val_files.csv', 'use_parallel': True, 'train_transform': 'none', 'num_workers': 12, 'dropout': 0.0, 'img_size': 224, 'class_weights': True, 'equalized_by': 'none', 'equalized_how': 'none', 'print_batches': True, 'scratch_dir': '/root/Documents/scratch', 'results_dir': './results/baseline', 'results_file': 'baseline'}\n",
      "Using cache found in ./download/pytorch_vision_v0.10.0\n",
      "Device: cuda #: 1 #cpus: 2\n",
      "\n",
      "Epoch 0\ttrain batch 0/384\n",
      "Epoch 0\ttrain batch 1/384\n",
      "Epoch 0\ttrain batch 2/384\n",
      "Epoch 0\ttrain batch 3/384\n",
      "Epoch 0\ttrain batch 4/384\n",
      "Epoch 0\ttrain batch 5/384\n",
      "Epoch 0\ttrain batch 6/384\n",
      "Epoch 0\ttrain batch 7/384\n",
      "Epoch 0\ttrain batch 8/384\n",
      "Epoch 0\ttrain batch 9/384\n",
      "Epoch 0\ttrain batch 10/384\n",
      "Epoch 0\ttrain batch 11/384\n",
      "Epoch 0\ttrain batch 12/384\n",
      "Epoch 0\ttrain batch 13/384\n",
      "Epoch 0\ttrain batch 14/384\n",
      "Epoch 0\ttrain batch 15/384\n",
      "Epoch 0\ttrain batch 16/384\n",
      "Epoch 0\ttrain batch 17/384\n",
      "Epoch 0\ttrain batch 18/384\n",
      "Epoch 0\ttrain batch 19/384\n",
      "Epoch 0\ttrain batch 20/384\n",
      "Epoch 0\ttrain batch 21/384\n",
      "Epoch 0\ttrain batch 22/384\n",
      "Epoch 0\ttrain batch 23/384\n",
      "Epoch 0\ttrain batch 24/384\n",
      "Epoch 0\ttrain batch 25/384\n",
      "Epoch 0\ttrain batch 26/384\n",
      "Epoch 0\ttrain batch 27/384\n",
      "Epoch 0\ttrain batch 28/384\n",
      "Epoch 0\ttrain batch 29/384\n",
      "Epoch 0\ttrain batch 30/384\n",
      "Epoch 0\ttrain batch 31/384\n",
      "Epoch 0\ttrain batch 32/384\n",
      "Epoch 0\ttrain batch 33/384\n",
      "Epoch 0\ttrain batch 34/384\n",
      "Epoch 0\ttrain batch 35/384\n",
      "Epoch 0\ttrain batch 36/384\n",
      "Epoch 0\ttrain batch 37/384\n",
      "Epoch 0\ttrain batch 38/384\n",
      "Epoch 0\ttrain batch 39/384\n",
      "Epoch 0\ttrain batch 40/384\n",
      "Epoch 0\ttrain batch 41/384\n",
      "Epoch 0\ttrain batch 42/384\n",
      "Epoch 0\ttrain batch 43/384\n",
      "Epoch 0\ttrain batch 44/384\n",
      "Epoch 0\ttrain batch 45/384\n",
      "Epoch 0\ttrain batch 46/384\n",
      "Epoch 0\ttrain batch 47/384\n",
      "Epoch 0\ttrain batch 48/384\n",
      "Epoch 0\ttrain batch 49/384\n",
      "Epoch 0\ttrain batch 50/384\n",
      "Epoch 0\ttrain batch 51/384\n",
      "Epoch 0\ttrain batch 52/384\n",
      "Epoch 0\ttrain batch 53/384\n",
      "Epoch 0\ttrain batch 54/384\n",
      "Epoch 0\ttrain batch 55/384\n",
      "Epoch 0\ttrain batch 56/384\n",
      "Epoch 0\ttrain batch 57/384\n",
      "Epoch 0\ttrain batch 58/384\n",
      "Epoch 0\ttrain batch 59/384\n",
      "Epoch 0\ttrain batch 60/384\n",
      "Epoch 0\ttrain batch 61/384\n",
      "Epoch 0\ttrain batch 62/384\n",
      "Epoch 0\ttrain batch 63/384\n",
      "Epoch 0\ttrain batch 64/384\n",
      "Epoch 0\ttrain batch 65/384\n",
      "Epoch 0\ttrain batch 66/384\n",
      "Epoch 0\ttrain batch 67/384\n",
      "Epoch 0\ttrain batch 68/384\n",
      "Epoch 0\ttrain batch 69/384\n",
      "Epoch 0\ttrain batch 70/384\n",
      "Epoch 0\ttrain batch 71/384\n",
      "Epoch 0\ttrain batch 72/384\n",
      "Epoch 0\ttrain batch 73/384\n",
      "Epoch 0\ttrain batch 74/384\n",
      "Epoch 0\ttrain batch 75/384\n",
      "Epoch 0\ttrain batch 76/384\n",
      "Epoch 0\ttrain batch 77/384\n",
      "Epoch 0\ttrain batch 78/384\n",
      "Epoch 0\ttrain batch 79/384\n",
      "Epoch 0\ttrain batch 80/384\n",
      "Epoch 0\ttrain batch 81/384\n",
      "Epoch 0\ttrain batch 82/384\n",
      "Epoch 0\ttrain batch 83/384\n",
      "Epoch 0\ttrain batch 84/384\n",
      "Epoch 0\ttrain batch 85/384\n",
      "Epoch 0\ttrain batch 86/384\n",
      "Epoch 0\ttrain batch 87/384\n",
      "Epoch 0\ttrain batch 88/384\n",
      "Epoch 0\ttrain batch 89/384\n",
      "Epoch 0\ttrain batch 90/384\n",
      "Epoch 0\ttrain batch 91/384\n",
      "Epoch 0\ttrain batch 92/384\n",
      "Epoch 0\ttrain batch 93/384\n",
      "Epoch 0\ttrain batch 94/384\n",
      "Epoch 0\ttrain batch 95/384\n",
      "Epoch 0\ttrain batch 96/384\n",
      "Epoch 0\ttrain batch 97/384\n",
      "Epoch 0\ttrain batch 98/384\n",
      "Epoch 0\ttrain batch 99/384\n",
      "Epoch 0\ttrain batch 100/384\n",
      "Epoch 0\ttrain batch 101/384\n",
      "Epoch 0\ttrain batch 102/384\n",
      "Epoch 0\ttrain batch 103/384\n",
      "Epoch 0\ttrain batch 104/384\n",
      "Epoch 0\ttrain batch 105/384\n",
      "Epoch 0\ttrain batch 106/384\n",
      "Epoch 0\ttrain batch 107/384\n",
      "Epoch 0\ttrain batch 108/384\n",
      "Epoch 0\ttrain batch 109/384\n",
      "Epoch 0\ttrain batch 110/384\n",
      "Epoch 0\ttrain batch 111/384\n",
      "Epoch 0\ttrain batch 112/384\n",
      "Epoch 0\ttrain batch 113/384\n",
      "Epoch 0\ttrain batch 114/384\n",
      "Epoch 0\ttrain batch 115/384\n",
      "Epoch 0\ttrain batch 116/384\n",
      "Epoch 0\ttrain batch 117/384\n",
      "Epoch 0\ttrain batch 118/384\n",
      "Epoch 0\ttrain batch 119/384\n",
      "Epoch 0\ttrain batch 120/384\n",
      "Epoch 0\ttrain batch 121/384\n",
      "Epoch 0\ttrain batch 122/384\n",
      "Epoch 0\ttrain batch 123/384\n",
      "Epoch 0\ttrain batch 124/384\n",
      "Epoch 0\ttrain batch 125/384\n",
      "Epoch 0\ttrain batch 126/384\n",
      "Epoch 0\ttrain batch 127/384\n",
      "Epoch 0\ttrain batch 128/384\n",
      "Epoch 0\ttrain batch 129/384\n",
      "Epoch 0\ttrain batch 130/384\n",
      "Epoch 0\ttrain batch 131/384\n",
      "Epoch 0\ttrain batch 132/384\n",
      "Epoch 0\ttrain batch 133/384\n",
      "Epoch 0\ttrain batch 134/384\n",
      "Epoch 0\ttrain batch 135/384\n",
      "Epoch 0\ttrain batch 136/384\n",
      "Epoch 0\ttrain batch 137/384\n",
      "Epoch 0\ttrain batch 138/384\n",
      "Epoch 0\ttrain batch 139/384\n",
      "Epoch 0\ttrain batch 140/384\n",
      "Epoch 0\ttrain batch 141/384\n",
      "Epoch 0\ttrain batch 142/384\n",
      "Epoch 0\ttrain batch 143/384\n",
      "Epoch 0\ttrain batch 144/384\n",
      "Epoch 0\ttrain batch 145/384\n",
      "Epoch 0\ttrain batch 146/384\n",
      "Epoch 0\ttrain batch 147/384\n",
      "Epoch 0\ttrain batch 148/384\n",
      "Epoch 0\ttrain batch 149/384\n",
      "Epoch 0\ttrain batch 150/384\n",
      "Epoch 0\ttrain batch 151/384\n",
      "Epoch 0\ttrain batch 152/384\n",
      "Epoch 0\ttrain batch 153/384\n",
      "Epoch 0\ttrain batch 154/384\n",
      "Epoch 0\ttrain batch 155/384\n",
      "Epoch 0\ttrain batch 156/384\n",
      "Epoch 0\ttrain batch 157/384\n",
      "Epoch 0\ttrain batch 158/384\n",
      "Epoch 0\ttrain batch 159/384\n",
      "Epoch 0\ttrain batch 160/384\n",
      "Epoch 0\ttrain batch 161/384\n",
      "Epoch 0\ttrain batch 162/384\n",
      "Epoch 0\ttrain batch 163/384\n",
      "Epoch 0\ttrain batch 164/384\n",
      "Epoch 0\ttrain batch 165/384\n",
      "Epoch 0\ttrain batch 166/384\n",
      "Epoch 0\ttrain batch 167/384\n",
      "Epoch 0\ttrain batch 168/384\n",
      "Epoch 0\ttrain batch 169/384\n",
      "Epoch 0\ttrain batch 170/384\n",
      "Epoch 0\ttrain batch 171/384\n",
      "Epoch 0\ttrain batch 172/384\n",
      "Epoch 0\ttrain batch 173/384\n",
      "Epoch 0\ttrain batch 174/384\n",
      "Epoch 0\ttrain batch 175/384\n",
      "Epoch 0\ttrain batch 176/384\n",
      "Epoch 0\ttrain batch 177/384\n",
      "Epoch 0\ttrain batch 178/384\n",
      "Epoch 0\ttrain batch 179/384\n",
      "Epoch 0\ttrain batch 180/384\n",
      "Epoch 0\ttrain batch 181/384\n",
      "Epoch 0\ttrain batch 182/384\n",
      "Epoch 0\ttrain batch 183/384\n",
      "Epoch 0\ttrain batch 184/384\n",
      "Epoch 0\ttrain batch 185/384\n",
      "Epoch 0\ttrain batch 186/384\n",
      "Epoch 0\ttrain batch 187/384\n",
      "Epoch 0\ttrain batch 188/384\n",
      "Epoch 0\ttrain batch 189/384\n",
      "Epoch 0\ttrain batch 190/384\n",
      "Epoch 0\ttrain batch 191/384\n",
      "Epoch 0\ttrain batch 192/384\n",
      "Epoch 0\ttrain batch 193/384\n",
      "Epoch 0\ttrain batch 194/384\n",
      "Epoch 0\ttrain batch 195/384\n",
      "Epoch 0\ttrain batch 196/384\n",
      "Epoch 0\ttrain batch 197/384\n",
      "Epoch 0\ttrain batch 198/384\n",
      "Epoch 0\ttrain batch 199/384\n",
      "Epoch 0\ttrain batch 200/384\n",
      "Epoch 0\ttrain batch 201/384\n",
      "Epoch 0\ttrain batch 202/384\n",
      "Epoch 0\ttrain batch 203/384\n",
      "Epoch 0\ttrain batch 204/384\n",
      "Epoch 0\ttrain batch 205/384\n",
      "Epoch 0\ttrain batch 206/384\n",
      "Epoch 0\ttrain batch 207/384\n",
      "Epoch 0\ttrain batch 208/384\n",
      "Epoch 0\ttrain batch 209/384\n",
      "Epoch 0\ttrain batch 210/384\n",
      "Epoch 0\ttrain batch 211/384\n",
      "Epoch 0\ttrain batch 212/384\n",
      "Epoch 0\ttrain batch 213/384\n",
      "Epoch 0\ttrain batch 214/384\n",
      "Epoch 0\ttrain batch 215/384\n",
      "Epoch 0\ttrain batch 216/384\n",
      "Epoch 0\ttrain batch 217/384\n",
      "Epoch 0\ttrain batch 218/384\n",
      "Epoch 0\ttrain batch 219/384\n",
      "Epoch 0\ttrain batch 220/384\n",
      "Epoch 0\ttrain batch 221/384\n",
      "Epoch 0\ttrain batch 222/384\n",
      "Epoch 0\ttrain batch 223/384\n",
      "Epoch 0\ttrain batch 224/384\n",
      "Epoch 0\ttrain batch 225/384\n",
      "Epoch 0\ttrain batch 226/384\n",
      "Epoch 0\ttrain batch 227/384\n",
      "Epoch 0\ttrain batch 228/384\n",
      "Epoch 0\ttrain batch 229/384\n",
      "Epoch 0\ttrain batch 230/384\n",
      "Epoch 0\ttrain batch 231/384\n",
      "Epoch 0\ttrain batch 232/384\n",
      "Epoch 0\ttrain batch 233/384\n",
      "Epoch 0\ttrain batch 234/384\n",
      "Epoch 0\ttrain batch 235/384\n",
      "Epoch 0\ttrain batch 236/384\n",
      "Epoch 0\ttrain batch 237/384\n",
      "Epoch 0\ttrain batch 238/384\n",
      "Epoch 0\ttrain batch 239/384\n",
      "Epoch 0\ttrain batch 240/384\n",
      "Epoch 0\ttrain batch 241/384\n",
      "Epoch 0\ttrain batch 242/384\n",
      "Epoch 0\ttrain batch 243/384\n",
      "Epoch 0\ttrain batch 244/384\n",
      "Epoch 0\ttrain batch 245/384\n",
      "Epoch 0\ttrain batch 246/384\n",
      "Epoch 0\ttrain batch 247/384\n",
      "Epoch 0\ttrain batch 248/384\n",
      "Epoch 0\ttrain batch 249/384\n",
      "Epoch 0\ttrain batch 250/384\n",
      "Epoch 0\ttrain batch 251/384\n",
      "Epoch 0\ttrain batch 252/384\n",
      "Epoch 0\ttrain batch 253/384\n",
      "Epoch 0\ttrain batch 254/384\n",
      "Epoch 0\ttrain batch 255/384\n",
      "Epoch 0\ttrain batch 256/384\n",
      "Epoch 0\ttrain batch 257/384\n",
      "Epoch 0\ttrain batch 258/384\n",
      "Epoch 0\ttrain batch 259/384\n",
      "Epoch 0\ttrain batch 260/384\n",
      "Epoch 0\ttrain batch 261/384\n",
      "Epoch 0\ttrain batch 262/384\n",
      "Epoch 0\ttrain batch 263/384\n",
      "Epoch 0\ttrain batch 264/384\n",
      "Epoch 0\ttrain batch 265/384\n",
      "Epoch 0\ttrain batch 266/384\n",
      "Epoch 0\ttrain batch 267/384\n",
      "Epoch 0\ttrain batch 268/384\n",
      "Epoch 0\ttrain batch 269/384\n",
      "Epoch 0\ttrain batch 270/384\n",
      "Epoch 0\ttrain batch 271/384\n",
      "Epoch 0\ttrain batch 272/384\n",
      "Epoch 0\ttrain batch 273/384\n",
      "Epoch 0\ttrain batch 274/384\n",
      "Epoch 0\ttrain batch 275/384\n",
      "Epoch 0\ttrain batch 276/384\n",
      "Epoch 0\ttrain batch 277/384\n",
      "Epoch 0\ttrain batch 278/384\n",
      "Epoch 0\ttrain batch 279/384\n",
      "Epoch 0\ttrain batch 280/384\n",
      "Epoch 0\ttrain batch 281/384\n",
      "Epoch 0\ttrain batch 282/384\n",
      "Epoch 0\ttrain batch 283/384\n",
      "Epoch 0\ttrain batch 284/384\n",
      "Epoch 0\ttrain batch 285/384\n",
      "Epoch 0\ttrain batch 286/384\n",
      "Epoch 0\ttrain batch 287/384\n",
      "Epoch 0\ttrain batch 288/384\n",
      "Epoch 0\ttrain batch 289/384\n",
      "Epoch 0\ttrain batch 290/384\n",
      "Epoch 0\ttrain batch 291/384\n",
      "Epoch 0\ttrain batch 292/384\n",
      "Epoch 0\ttrain batch 293/384\n",
      "Epoch 0\ttrain batch 294/384\n",
      "Epoch 0\ttrain batch 295/384\n",
      "Epoch 0\ttrain batch 296/384\n",
      "Epoch 0\ttrain batch 297/384\n",
      "Epoch 0\ttrain batch 298/384\n",
      "Epoch 0\ttrain batch 299/384\n",
      "Epoch 0\ttrain batch 300/384\n",
      "Epoch 0\ttrain batch 301/384\n",
      "Epoch 0\ttrain batch 302/384\n",
      "Epoch 0\ttrain batch 303/384\n",
      "Epoch 0\ttrain batch 304/384\n",
      "Epoch 0\ttrain batch 305/384\n",
      "Epoch 0\ttrain batch 306/384\n",
      "Epoch 0\ttrain batch 307/384\n",
      "Epoch 0\ttrain batch 308/384\n",
      "Epoch 0\ttrain batch 309/384\n",
      "Epoch 0\ttrain batch 310/384\n",
      "Epoch 0\ttrain batch 311/384\n",
      "Epoch 0\ttrain batch 312/384\n",
      "Epoch 0\ttrain batch 313/384\n",
      "Epoch 0\ttrain batch 314/384\n",
      "Epoch 0\ttrain batch 315/384\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa8d2b85430>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1456, in _shutdown_workers\n",
      "    self._pin_memory_thread.join()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "  File \"train_all.py\", line 413, in <module>\n",
      "    main()\n",
      "  File \"train_all.py\", line 266, in main\n",
      "    yhat = model(x)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/data_parallel.py\", line 166, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\", line 285, in forward\n",
      "    return self._forward_impl(x)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\", line 280, in _forward_impl\n",
      "    x = self.fc(x)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!python3 predict.py --model-dir \"./results/baseline\" --model-file \"baseline_model.pt\" --model-name \"baseline\" --architecture \"ResNet18\" --results-dir \"./results/baseline\"\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbE-GyXMWKpP",
    "outputId": "40c00aef-fd42-42f5-af66-c2a65986d25a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'model_type': 'ResNet18', 'pretrained': True, 'n_labels': 7, 'frozen': False, 'batch_size': 16, 'data_dir': './data', 'dataset': 'RAF', 'test_file': 'test_files.csv', 'use_parallel': True, 'num_workers': 12, 'img_size': 224, 'print_batches': False, 'scratch_dir': '/root/Documents/scratch', 'results_dir': './results/baseline', 'results_file': './results/baseline/baseline_predicttest_files.csv.npz'}\n",
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to ./download/v0.10.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to ./download/checkpoints/resnet18-f37072fd.pth\n",
      "100% 44.7M/44.7M [00:01<00:00, 36.7MB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"predict.py\", line 94, in <module>\n",
      "    model = RAF.load_model(model_path, model_args)\n",
      "  File \"/content/frowning-bias/custom_modules.py\", line 132, in load_model\n",
      "    model.load_state_dict(T.load(model_path))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 699, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 211, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './results/baseline/baseline_model.pt'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! python3 eval.py --predictions-dir \"./results/baseline\" --predictions-file \"baseline_predicttest_files.csv.npz\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBNZ6MSPXc_G",
    "outputId": "3da4dc91-57dd-47e2-b6b7-e47645bb4d4a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"eval.py\", line 45, in <module>\n",
      "    predictions = np.load(predictions_file)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\", line 417, in load\n",
      "    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './results/baseline/baseline_predicttest_files.csv.npz'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "mq4PukTsXric",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}